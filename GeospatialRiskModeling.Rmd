---
title: "Geospatial Risk Modeling: Thefts from Automobiles in Washington, DC"
author: "Jenna Epstein"
date: "10/23/2020"
output: html_document
---
# Setup
```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)

library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(mapview)

# functions
#root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

nn_function <- function(measureFrom,measureTo,k) {
  measureFrom_Matrix <-
    as.matrix(measureFrom)
  measureTo_Matrix <-
    as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
    output <-
      as.data.frame(nn) %>%
      rownames_to_column(var = "thisPoint") %>%
      gather(points, point_distance, V1:ncol(.)) %>%
      arrange(as.numeric(thisPoint)) %>%
      group_by(thisPoint) %>%
      summarize(pointDistance = mean(point_distance)) %>%
      arrange(as.numeric(thisPoint)) %>% 
      dplyr::select(-thisPoint) %>%
      pull()
  
  return(output)  
}

#r cross validate function

crossValidate <- function(dataset, id, dependentVariable, indVariables) {

allPredictions <- data.frame()
cvID_list <- unique(dataset[[id]])

for (i in cvID_list) {

  thisFold <- i
  cat("This hold out fold is", thisFold, "\n")

  fold.train <- filter(dataset, dataset[[id]] != thisFold) %>% as.data.frame() %>% 
                dplyr::select(id, geometry, indVariables, dependentVariable)
  fold.test  <- filter(dataset, dataset[[id]] == thisFold) %>% as.data.frame() %>% 
                dplyr::select(id, geometry, indVariables, dependentVariable)
  
  regression <-
    glm(count_theftsFromAuto ~ ., family = "poisson", 
      data = fold.train %>% 
      dplyr::select(-geometry, -id))
  
  thisPrediction <- 
    mutate(fold.test, Prediction = predict(regression, fold.test, type = "response"))
    
  allPredictions <-
    rbind(allPredictions, thisPrediction)
    
  }
  return(st_sf(allPredictions))
}

mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.text.x = element_text(size = 14))
}

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}
qBr <- function(df, variable, rnd) {
  if (missing(rnd)) {
    as.character(quantile(round(df[[variable]],0),
                          c(.01,.2,.4,.6,.8), na.rm=T))
  } else if (rnd == FALSE | rnd == F) {
    as.character(formatC(quantile(df[[variable]]), digits = 3),
                 c(.01,.2,.4,.6,.8), na.rm=T)
  }
}

q5 <- function(variable) {as.factor(ntile(variable, 5))}

```


# Data Wrangling

## Outcome Variable: Thefts from Automobiles

Text here.

```{r read data}

#DC police boundary data
policeSectors <- 
  st_read("https://opendata.arcgis.com/datasets/6ac17c2ff8cc4e20b3768dd1b98adf7a_23.geojson") %>%
  st_transform('ESRI:102685') %>%
  dplyr::select(Area = SECTOR)
  
policeServiceAreas <- 
  st_read("https://opendata.arcgis.com/datasets/db24f3b7de994501aea97ce05a50547e_10.geojson") %>%
  st_transform('ESRI:102685') %>%
  dplyr::select(Area = NAME)

bothPoliceUnits <- rbind(mutate(policeSectors, Legend = "Police Sectors"), 
                         mutate(policeServiceAreas, Legend = "Police Service Areas"))

#outcome variable
theftsFromAuto <- 
  st_read("https://opendata.arcgis.com/datasets/38ba41dd74354563bce28a359b59324e_0.geojson") %>% 
    filter(OFFENSE == "THEFT F/AUTO") %>%
    mutate(X = as.numeric(LONGITUDE),Y = as.numeric(LATITUDE)) %>% 
    filter(!is.na(X)) %>%
    filter(!is.na(Y)) %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant")%>%
    st_transform('ESRI:102685') %>% 
    distinct()

#DC boundary
boundaryDC <-
  st_read("https://opendata.arcgis.com/datasets/7241f6d500b44288ad983f0942b39663_10.geojson") %>%
  st_transform('ESRI:102685')

```

```{r fig.width=9, fig.height=5}
# uses grid.arrange to organize independent plots
grid.arrange(ncol=2,
ggplot() + 
  geom_sf(data = boundaryDC) +
  geom_sf(data = theftsFromAuto, colour="black", size=0.1, show.legend = "point") +
  labs(title= "Theft from Motor Vehicles, 2018",
       subtitle="Washington, DC") +
  mapTheme(),

ggplot() + 
  geom_sf(data = boundaryDC, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(theftsFromAuto)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 50, geom = 'polygon') +
  scale_fill_viridis() +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Thefts from Motor Vehicles, 2018",
       subtitle = "Washington, DC") +
  mapTheme() + theme(legend.position = "none"))

```

Text here about joining incidents to the fishnet grid.

```{r fishnet grid, message=FALSE, warning=FALSE}
fishnet <- 
  st_make_grid(boundaryDC,
               cellsize = 500, 
               square = TRUE) %>%
  .[boundaryDC] %>% 
  st_sf() %>%
  mutate(uniqueID = rownames(.))
```

```{r aggregate points into fishnet grid, message=FALSE, warning=FALSE}
## add a value of 1 to each crime, sum them with aggregate
theftsFromAuto_net <- 
  dplyr::select(theftsFromAuto) %>% 
  mutate(count_theftsFromAuto = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(count_theftsFromAuto = replace_na(count_theftsFromAuto, 0),
         uniqueID = rownames(.),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE))

ggplot() +
  geom_sf(data = theftsFromAuto_net, aes(fill = count_theftsFromAuto), color = NA) +
  scale_fill_viridis() +
  labs(title = "Count of Theft Auto Incidents for the Fishnet") +
  mapTheme()
```

## Risk Factors

Text here all about the risk factors chosen to help with predictions.
Also reading in the neighborhood data at this point.

```{r data wrangling risk factors}
## using 311 dataset for all requests in DC in 2018
abandonedCars <- 
  st_read("https://opendata.arcgis.com/datasets/2a46f1f1aad04940b83e75e744eb3b09_9.geojson") %>%
  filter(SERVICECODEDESCRIPTION == "Abandoned Vehicle - On Public Property") %>%  
  dplyr::select(Y = LATITUDE, X = LONGITUDE) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Abandoned_Cars")

graffiti <- 
  st_read("https://opendata.arcgis.com/datasets/2a46f1f1aad04940b83e75e744eb3b09_9.geojson") %>%
  filter(SERVICECODEDESCRIPTION == "Graffiti Removal") %>%  
  dplyr::select(Y = LATITUDE, X = LONGITUDE) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Graffiti")

illegalDumping <- 
  st_read("https://opendata.arcgis.com/datasets/2a46f1f1aad04940b83e75e744eb3b09_9.geojson") %>%
  filter(SERVICECODEDESCRIPTION == "Illegal Dumping") %>%  
  dplyr::select(Y = LATITUDE, X = LONGITUDE) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Illegal Dumping")

parkingEnf <- 
  st_read("https://opendata.arcgis.com/datasets/2a46f1f1aad04940b83e75e744eb3b09_9.geojson") %>%
  filter(SERVICECODEDESCRIPTION == "Parking Enforcement") %>%  
  dplyr::select(Y = LATITUDE, X = LONGITUDE) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Parking Enforcement")
 
sanitationEnf <- 
  st_read("https://opendata.arcgis.com/datasets/2a46f1f1aad04940b83e75e744eb3b09_9.geojson") %>%
  filter(SERVICECODEDESCRIPTION == "Sanitation Enforcement") %>%  
  dplyr::select(Y = LATITUDE, X = LONGITUDE) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Sanitation Enforcement")

streetlights <- 
  st_read("https://opendata.arcgis.com/datasets/2a46f1f1aad04940b83e75e744eb3b09_9.geojson") %>%
  filter(SERVICECODEDESCRIPTION == "Streetlight Repair Investigation") %>%  
  dplyr::select(Y = LATITUDE, X = LONGITUDE) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Streetlight Repair Investigation")

vacantLots <- 
  st_read("https://opendata.arcgis.com/datasets/2a46f1f1aad04940b83e75e744eb3b09_9.geojson") %>%
  filter(SERVICECODEDESCRIPTION == "Vacant Lot - Public Property Only") %>%  
  dplyr::select(Y = LATITUDE, X = LONGITUDE) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Vacant Lots")

## different open data dc dataset
liquorStores <- 
  st_read("https://opendata.arcgis.com/datasets/cabe9dcef0b344518c7fae1a3def7de1_5.geojson") %>%
  filter(TYPE == "Retail - Liquor Store") %>%  
  dplyr::select(Y = LATITUDE, X = LONGITUDE) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Liquor Stores")

```

```{r neighborhoods}
#Reading in DC Neighborhoods Spatial Data
neighborhoods <- 
  st_read("https://opendata.arcgis.com/datasets/de63a68eb7674548ae0ac01867123f7e_13.geojson") %>%
  dplyr::select(OBJECTID, DC_HPN_NAME, geometry) %>%
  dplyr::rename(NAME = DC_HPN_NAME) %>%
  st_transform(st_crs(fishnet)) 

```
# Feature Engineering

## Count of Risk Factors by Fishnet Grid Cell
Text goes here.

```{r aggregate features to fishnet, message=FALSE, warning=FALSE}
## Aggregating Risk Factors (Features) to the Fishnet Grid using Nearest Neighbor
vars_net <- 
rbind(abandonedCars, graffiti, sanitationEnf, parkingEnf, streetlights, vacantLots, illegalDumping, liquorStores) %>%
  st_join(., fishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>%
    full_join(fishnet, by="uniqueID") %>%
    spread(Legend, count, fill=0) %>%
    st_sf() %>%
    dplyr::select(-`<NA>`) %>%
    na.omit() %>%
    ungroup()

```

```{r feature engineering - count of risk factors by grid cell, eval=FALSE, include=FALSE}
vars_net.long <- 
  gather(vars_net, Variable, value, -geometry, -uniqueID)

###plotting
#vars <- unique(vars_net.long$Variable)
#mapList <- list()

#for(i in vars){
#  mapList[[i]] <- 
#    ggplot() +
#      geom_sf(data = filter(vars_net.long, Variable == i), aes(fill=value), colour=NA) +
#      scale_fill_viridis(name="") +
#      labs(title=i) +
#      mapTheme()}

#do.call(grid.arrange,c(mapList, ncol =3, top = "Risk Factors by Fishnet"))
```

## Nearest Neighbor Features

```{r feature engineering - nearest neighbor features}
st_c <- st_coordinates
st_coid <- st_centroid

vars_net <-
  vars_net %>%
    mutate(
      Abandoned_Cars.nn =
        nn_function(st_c(st_coid(vars_net)), st_c(abandonedCars),3),
      Graffiti.nn =
        nn_function(st_c(st_coid(vars_net)), st_c(graffiti),3),
      Illegal_Dumping.nn =
        nn_function(st_c(st_coid(vars_net)), st_c(illegalDumping),3),
      Liquor_Retail.nn =
        nn_function(st_c(st_coid(vars_net)), st_c(liquorStores),3),
      Parking_Enforcement.nn =
        nn_function(st_c(st_coid(vars_net)), st_c(parkingEnf),3),
      Sanitation_Enforcement.nn =
        nn_function(st_c(st_coid(vars_net)), st_c(sanitationEnf),3),
      Streetlight_Repair.nn =
        nn_function(st_c(st_coid(vars_net)), st_c(streetlights),3),
      Vacant_Lots.nn =
        nn_function(st_c(st_coid(vars_net)), st_c(vacantLots),3))
```

```{r feature engineering - nearest neighbor features2, fig.width=10, message=FALSE, warning=FALSE}
vars_net.long.nn <- 
  dplyr::select(vars_net, ends_with(".nn")) %>%
    gather(Variable, value, -geometry)

vars <- unique(vars_net.long.nn$Variable)
mapList <- list()

for(i in vars){
  mapList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(vars_net.long.nn, Variable == i), aes(fill=value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme()}

do.call(grid.arrange,c(mapList, ncol = 3, top = "Nearest Neighbor Risk Factors by Fishnet Grid Cell"))
```

## Creating the Final Fishnet Grid
```{r Feature Engineering - Create the final_net, message=FALSE, warning=FALSE}
final_net <-
  left_join(theftsFromAuto_net, st_drop_geometry(vars_net), by="uniqueID") 

final_net <-
  st_centroid(final_net) %>%
    st_join(dplyr::select(neighborhoods, NAME)) %>%
    st_join(dplyr::select(policeSectors, Area)) %>%
      st_drop_geometry() %>%
      left_join(dplyr::select(final_net, geometry, uniqueID)) %>%
      st_sf() %>%
  na.omit()

```

# Exploring the Spatial Process of Theft from Automobiles
Text goes here.

## Local Moran's I
Text goes here.

```{r local Morans I prep, message=FALSE, warning=FALSE}
## {spdep} to make polygon to neighborhoods
final_net.nb <- poly2nb(as_Spatial(final_net), queen=TRUE)
## and neighborhoods to list of weights
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE)

```

```{r local Morans I, message=FALSE, warning=FALSE}
final_net.localMorans <- 
  cbind(
    as.data.frame(localmoran(final_net$count_theftsFromAuto, final_net.weights)),
    as.data.frame(final_net)) %>% 
    st_sf() %>%
      dplyr::select(theftsFromAuto_Count = count_theftsFromAuto, 
                    Local_Morans_I = Ii, 
                    P_Value = `Pr(z > 0)`) %>%
      mutate(Significant_Hotspots = ifelse(P_Value <= 0.05, 1, 0)) %>%
      gather(Variable, Value, -geometry)

```

### Plotting Local Moran's I results
```{r plotting local Morans I, message=FALSE, warning=FALSE}
vars2 <- unique(final_net.localMorans$Variable)
varList <- list()

for(i in vars2){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme() + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 4, top = "Local Morans I statistics, Thefts from Auto"))

```

### Plotting Nearest Neighbor Distance to Hotspots
The map below shows the distance to highly significant hotspots for thefts from automobiles. This is done by creating a local Moran's I feature in the final fishnet grid, using a dummy variable to indicate that a cell is part of a significant cluster, and then measuring measuring the average nearest neighbor distance from the centroid of each grid cell to its nearest significant cluster. A p-value of 0.0000001 was chosen Since the smaller the p-value, the more significant the clusters. After running some variations, the p-value chosen here is 0.0000001.

```{r plotting distance to significant hotspots, message=FALSE, warning=FALSE}
# first, create a Local Moran's I feature in final_net
final_net <-
  final_net %>% 
  mutate(theftsFromAuto.isSig = 
           ifelse(localmoran(final_net$count_theftsFromAuto, 
                             final_net.weights)[,5] <= 0.0000001, 1, 0)) %>%
  mutate(theftsFromAuto.isSig.dist = 
           nn_function(st_coordinates(st_centroid(final_net)),
                       st_coordinates(st_centroid(
                         filter(final_net, theftsFromAuto.isSig == 1))), 1))


ggplot() +
      geom_sf(data = final_net, aes(fill=theftsFromAuto.isSig.dist), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Distance to Highly Significant Theft from Auto Hotspots") +
      mapTheme()
```

## Correlation Tests
The scatterplots below show the count and nearest neighbor correlations. When running the regressions later, it is best to pick either a count risk factor variable or a nearest neighbor risk factor variable - including both will lead to colinearity. Based on these tests, I select the nearest neighbor risk factors [ADD THE WHY]

```{r fig.height=12, message=FALSE, warning=FALSE}

correlation.long <-
  st_drop_geometry(final_net) %>%
    dplyr::select(-uniqueID, -cvID, -NAME, -Area) %>%
    gather(Variable, Value, -count_theftsFromAuto)

correlation.cor <-
  correlation.long %>%
    group_by(Variable) %>%
    summarize(correlation = cor(Value, count_theftsFromAuto, use = "complete.obs"))
    
ggplot(correlation.long, aes(Value, count_theftsFromAuto)) +
  geom_point(size = 0.1) +
  geom_text(data = correlation.cor, aes(label = paste("r =", round(correlation, 2))),
            x=-Inf, y=Inf, vjust = 1.25, hjust = -0.1, colour="blue", fontface ="bold") +
  geom_smooth(method = "lm", se = FALSE, colour = "black") +
  facet_wrap(~Variable, ncol = 2, scales = "free") +
  labs(title = "Thefts from Auto count as a function of risk factors") +
  plotTheme()
```

## Histogram of Outcome Variable
In this histogram below showing distribution of thefts from automobiles, one can see that the majority of grid cells have 0 or 1 incident of this crime type, indicating that this crime type is relatively rare. Given this skewed distribution towards low frequency, an ordinary least squares (OLS) regression is not the best approach. Instead, a Poisson regression - a regression used in modeling an outcome variable consisting of count data - is more useful here.

```{r histogram of outcome variable, message=FALSE, warning=FALSE}
# histogram of outcome variable
ggplot(final_net, aes(count_theftsFromAuto)) +
  geom_histogram(binwidth=1, color="grey40") +
  stat_bin(binwidth=1, geom="label", color="black", bins=6, aes(label=..count..), position=position_stack(vjust=0.9)) +
  labs(title="Distribution of Thefts from Automobiles",
       y="Count of Cells", x = "Count of Thefts From Automobiles") + plotTheme()

```

# Poisson Regression
## Cross-validated Poisson Regression

To train the predictive model, the "leave-one=group-out" spatial cross-validation approach (LOGO-CV) is used. By training the model on all areas except for one at a time - predicting the hold out and recording the goodness of fit for each - LOGO-CV rotates through and trains the model across various spatial scales.

Below, four regressions are run using the CrossValidate function, which incorporates the count_theftsFromAuto variable. Two regressions are run for risk factors ("reg.cv" and "reg.spatialCV", and two are run for risk factors including the spatial process features engineered previously using the local Moran's I ("reg.ss.cv" and reg.ss.spatialCV).

```{r message=FALSE, warning=FALSE}
# just risk factors (reg.vars)
reg.vars <- c("Abandoned_Cars.nn", "Graffiti.nn", "Illegal_Dumping.nn", "Liquor_Retail.nn", "Parking_Enforcement.nn", "Sanitation_Enforcement.nn", "Streetlight_Repair.nn", "Vacant_Lots.nn")
```

```{r}
## risk factors plus local morans I spatial process features (reg.ss.vars)
reg.ss.vars <- c("Abandoned_Cars.nn", "Graffiti.nn", "Illegal_Dumping.nn", "Liquor_Retail.nn", "Parking_Enforcement.nn", "Sanitation_Enforcement.nn", "Streetlight_Repair.nn", "Vacant_Lots.nn", "theftsFromAuto.isSig", "theftsFromAuto.isSig.dist")
```

```{r message=FALSE, warning=FALSE}
## run regressions
reg.cv <- crossValidate(
  dataset = final_net,
  id = "cvID",
  dependentVariable = "count_theftsFromAuto",
  indVariables = reg.vars) %>%
    dplyr::select(cvID = cvID, count_theftsFromAuto, Prediction, geometry)

reg.ss.cv <- crossValidate(
  dataset = final_net,
  id = "cvID",
  dependentVariable = "count_theftsFromAuto",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = cvID, count_theftsFromAuto, Prediction, geometry)
  
reg.spatialCV <- crossValidate(
  dataset = final_net,
  id = "NAME",
  dependentVariable = "count_theftsFromAuto",
  indVariables = reg.vars) %>%
    dplyr::select(cvID = NAME, count_theftsFromAuto, Prediction, geometry)

reg.ss.spatialCV <- crossValidate(
  dataset = final_net,
  id = "NAME",
  dependentVariable = "count_theftsFromAuto",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = NAME, count_theftsFromAuto, Prediction, geometry)
```

## Accuracy and Generalizability

To test the accuracy and generalizability across space, I calculate the mean absolute error (MAE) for each fold for each of the regressions run. The set of histograms below show that the LOGO-CV regressions - both for just the risk factors and accounting for spatial process- reduce errors.

```{r regression summary, message=FALSE, warning=FALSE}
reg.summary <- 
  rbind(
    mutate(reg.cv,           Error = Prediction - count_theftsFromAuto,
                             Regression = "Random k-fold CV: Just Risk Factors"),
                             
    mutate(reg.ss.cv,        Error = Prediction - count_theftsFromAuto,
                             Regression = "Random k-fold CV: Spatial Process"),
    
    mutate(reg.spatialCV,    Error = Prediction - count_theftsFromAuto,
                             Regression = "Spatial LOGO-CV: Just Risk Factors"),
                             
    mutate(reg.ss.spatialCV, Error = Prediction - count_theftsFromAuto,
                             Regression = "Spatial LOGO-CV: Spatial Process")) %>%
    st_sf() 
```

```{r calculate and plot errors, message=FALSE, warning=FALSE}
# calculate errors
error_by_reg_and_fold <- 
  reg.summary %>%
    group_by(Regression, cvID) %>% 
    summarize(Mean_Error = mean(Prediction - count_theftsFromAuto, na.rm = T),
              MAE = mean(abs(Mean_Error), na.rm = T),
              SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

## plot histogram of OOF (out of fold) errors
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="black", fill = "#FDE725FF") +
    facet_wrap(~Regression) +  
    geom_vline(xintercept = 0) + scale_x_continuous(breaks = seq(0, 8, by = 1)) + 
    labs(title="Distribution of MAE", subtitle = "k-fold cross validation vs. LOGO-CV",
         x="Mean Absolute Error", y="Count") 
```
```{r message=FALSE, warning=FALSE}
st_drop_geometry(error_by_reg_and_fold) %>%
  group_by(Regression) %>% 
    summarize(Mean_MAE = round(mean(MAE), 2),
              SD_MAE = round(sd(MAE), 2)) %>%
  kable(caption = "Mean Absolute Error, By Regression") %>%
    kable_styling("striped", full_width = F) %>%
    row_spec(2, color = "black", background = "#FDE725FF") %>%
    row_spec(4, color = "black", background = "#FDE725FF") 
```
```{r error by reg and fold, message=FALSE, warning=FALSE}
error_by_reg_and_fold %>%
  filter(str_detect(Regression, "LOGO")) %>%
  ggplot() +
    geom_sf(aes(fill = MAE)) +
    facet_wrap(~Regression) +
    scale_fill_viridis() +
    labs(title = "Thefts from Auto Errors, by LOGO-CV Regression") +
    mapTheme() + theme(legend.position="bottom")
```
```{r message=FALSE, warning=FALSE}
neighborhood.weights <-
  filter(error_by_reg_and_fold, Regression == "Spatial LOGO-CV: Spatial Process") %>%
    group_by(cvID) %>%
      poly2nb(as_Spatial(.), queen=TRUE) %>%
      nb2listw(., style="W", zero.policy=TRUE)

filter(error_by_reg_and_fold, str_detect(Regression, "LOGO"))  %>% 
    st_drop_geometry() %>%
    group_by(Regression) %>%
    summarize(Morans_I = moran.mc(abs(Mean_Error), neighborhood.weights, 
                                 nsim = 999, zero.policy = TRUE, 
                                 na.action=na.omit)[[1]],
              p_value = moran.mc(abs(Mean_Error), neighborhood.weights, 
                                 nsim = 999, zero.policy = TRUE, 
                                 na.action=na.omit)[[3]]) %>% kable() %>% kable_styling()
```
```{r pred and observed, message=FALSE, warning=FALSE}
st_drop_geometry(reg.summary) %>%
  group_by(Regression) %>%
    mutate(theftsFromAuto_Decile = ntile(count_theftsFromAuto, 10)) %>%
  group_by(Regression, theftsFromAuto_Decile) %>%
    summarize(meanObserved = mean(count_theftsFromAuto, na.rm=T),
              meanPrediction = mean(Prediction, na.rm=T)) %>%
    gather(Variable, Value, -Regression, -theftsFromAuto_Decile) %>%          
    ggplot(aes(theftsFromAuto_Decile, Value, shape = Variable)) +
      geom_point(size = 2) + geom_path(aes(group = theftsFromAuto_Decile), colour = "black") +
      scale_shape_manual(values = c(2, 17)) +
      facet_wrap(~Regression) + xlim(0,10) +
      labs(title = "Predicted and observed thefts by Auto by observed thefts by auto decile")
```

# Generalizability by Neighborhood Context

```{r tidycensus, message=FALSE, warning=FALSE}

census_api_key("41e1c0d912341017fa6f36a5da061d3b23de335e", overwrite = TRUE)

tracts2018 <- 
  get_acs(geography = "tract", variables = c("B01001_001E","B01001A_001E"), 
          year = 2018, state=11, geometry=T) %>%
  st_transform('ESRI:102685')  %>% 
  dplyr::select(variable, estimate, GEOID) %>%
  spread(variable, estimate) %>%
  rename(TotalPop = B01001_001,
         NumberWhites = B01001A_001) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority_White", "Majority_Non_White")) %>%
  .[neighborhoods,]

```
```{r mean error by nhood racial context, message=FALSE, warning=FALSE}
reg.summary %>% 
  filter(str_detect(Regression, "LOGO")) %>%
    st_centroid() %>%
    st_join(tracts2018) %>%
    na.omit() %>%
      st_drop_geometry() %>%
      group_by(Regression, raceContext) %>%
      summarize(mean.Error = mean(Error, na.rm = T)) %>%
      spread(raceContext, mean.Error) %>%
      kable(caption = "Mean Error by neighborhood racial context") %>%
        kable_styling("striped", full_width = F) 
```
## Do Risk Predictions Outperform Kernel Density Hotspot Mapping?

```{r kernal density, message=FALSE, warning=FALSE}

spatstat.options(npixel=c(256,256))

tfa_ppp <- as.ppp(st_coordinates(theftsFromAuto), W = st_bbox(final_net))
tfa_KD.1000 <- spatstat::density.ppp(tfa_ppp, 1000)
tfa_KD.1500 <- spatstat::density.ppp(tfa_ppp, 1500)
tfa_KD.2000 <- spatstat::density.ppp(tfa_ppp, 2000)
tfa_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(tfa_KD.1000), as(neighborhoods, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(tfa_KD.1500), as(neighborhoods, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(tfa_KD.2000), as(neighborhoods, 'Spatial')))), Legend = "2000 Ft.")) 

tfa_KD.df$Legend <- factor(tfa_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

ggplot(data=tfa_KD.df, aes(x=x, y=y)) +
  geom_raster(aes(fill=layer)) + 
  facet_wrap(~Legend) +
  coord_sf(crs=st_crs(final_net)) + 
  scale_fill_viridis(name="Density") +
  labs(title = "Kernel Density with Three Different Search Radii") +
  mapTheme()
```

```{r eval=FALSE, warning=FALSE, include=FALSE}
#first, convert to raster using raster(tfa_KD.1000)
#then resample to smaller grid size. Used aggregate() isntead since resample() didn't see to be working. Used factor c(2) to take it down from 128x128 to 64x64.
#then, use raster::extract (zonal stats)

#convert to raster
#tfa_KD.1000.raster <- raster(tfa_KD.1000) 
#tfa_KD.1000.raster2 <- aggregate(tfa_KD.1000.raster, fact=c(2))
#ncol(tfa_KD.1000.raster2)
#nrow(tfa_KD.1000.raster2)

#tfa_KD.1000.extract <- raster::extract(tfa_KD.1000.raster2, final_net, mean, na.rm=TRUE, sp=TRUE)

  
#tfa_KD.1000.sf <- st_as_sf(tfa_KD.1000.extract)
#tfa_KD.raster <- raster(tfa_KD) 
#tfa_KD.raster2 <- aggregate(tfa_KD.raster, fact=c(2))
#ncol(tfa_KD.raster2)
#nrow(tfa_KD.raster2)

#tfa_KD.extract <- raster::extract(tfa_KD.raster2, final_net, mean, na.rm=TRUE, sp=TRUE)


```

```{r kernel density 2018 thefts, message=FALSE, warning=FALSE}

tfa_KD_ppp <- as.ppp(st_coordinates(theftsFromAuto), W = st_bbox(final_net))
tfa_KD <- spatstat::density.ppp(tfa_KD_ppp, 1000)


kden <- as.data.frame(tfa_KD)  %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean)

  ggplot(kden) +
     geom_sf(aes(fill=value)) +
     geom_sf(data = sample_n(theftsFromAuto, 1500), size = .5) +
     scale_fill_viridis(name = "Density") +
     labs(title = "Kernel Density of 2018 Thefts from Automobiles") +
     mapTheme()
   
```

```{r read in 2019 thefts from auto data, message=FALSE, warning=FALSE}
#new goodness of fit indicate to see whether 2018 kernel density or risk prediction captures more of the 2019 burglaries 

theftsFromAuto2019 <- 
  st_read("https://opendata.arcgis.com/datasets/f08294e5286141c293e9202fcd3e8b57_1.geojson") %>% 
    filter(OFFENSE == "THEFT F/AUTO") %>%
    mutate(X = as.numeric(LONGITUDE),Y = as.numeric(LATITUDE)) %>% 
    filter(!is.na(X)) %>%
    filter(!is.na(Y)) %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant")%>%
    st_transform('ESRI:102685') %>% 
    distinct() %>%
    .[fishnet,]

```
## Comparison of Kernel Density and Risk Predictions

```{r}
tfa_KDE_sf <- as.data.frame(tfa_KD) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
  mutate(label = "Kernel Density",
         Risk_Category = ntile(value, 100),
         Risk_Category = case_when(
           Risk_Category >= 90 ~ "90% to 100%",
           Risk_Category >= 70 & Risk_Category <= 89 ~ "70% to 89%",
           Risk_Category >= 50 & Risk_Category <= 69 ~ "50% to 69%",
           Risk_Category >= 30 & Risk_Category <= 49 ~ "30% to 49%",
           Risk_Category >= 1 & Risk_Category <= 29 ~ "1% to 29%")) %>%
  cbind(
    aggregate(
      dplyr::select(theftsFromAuto2019) %>% mutate(tfaCount = 1), ., sum) %>%
    mutate(tfaCount = replace_na(tfaCount, 0))) %>%
  dplyr::select(label, Risk_Category, tfaCount)
```

```{r}
tfa_risk_sf <-
  reg.ss.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category = ntile(Prediction, 100),
         Risk_Category = case_when(
         Risk_Category >= 90 ~ "90% to 100%",
         Risk_Category >= 70 & Risk_Category <= 89 ~ "70% to 89%",
         Risk_Category >= 50 & Risk_Category <= 69 ~ "50% to 69%",
         Risk_Category >= 30 & Risk_Category <= 49 ~ "30% to 49%",
         Risk_Category >= 1 & Risk_Category <= 29 ~ "1% to 29%")) %>%
  cbind(
    aggregate(
      dplyr::select(theftsFromAuto2019) %>% mutate(tfaCount = 1), ., sum) %>%
      mutate(tfaCount = replace_na(tfaCount, 0))) %>%
  dplyr::select(label, Risk_Category, tfaCount)
```

```{r}
rbind(tfa_KDE_sf, tfa_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(theftsFromAuto2019, 3000), size = .5, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2018 Thefts from Automobiles - Risk Predictions; 2019 Thefts from Automobiles Overlay") +
    mapTheme()
```
```{r kernel density and risk predcitions histogram, message=FALSE, warning=FALSE}
rbind(tfa_KDE_sf, tfa_risk_sf) %>%
  st_set_geometry(NULL) %>% na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(count_theftsFromAuto = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Rate_of_test_set_crimes = count_theftsFromAuto / sum(count_theftsFromAuto)) %>%
    ggplot(aes(Risk_Category,Rate_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE) +
      labs(title = "Risk Prediction vs. Kernel Density, 2019 Thefts from Automobiles") +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```

